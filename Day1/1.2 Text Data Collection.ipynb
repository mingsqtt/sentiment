{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Collection\n",
    "\n",
    "To do analysis in text, you need textual data! The sources of these data is varied. Some of them are for academic purposes - well labelled etc. But the truth is often you need to collect from the 'real-world'. These include collection from the Internet - RSS sites, google pages, social media etc. \n",
    "\n",
    "These form an important avenue to collect data from the Internet to do sentiment analysis. For eg. almost all news media provide RSS. Note that RSS is not UGC, and thence differences can be expected from social media or blogs. The content and how it is written are substantially different from 'short messages'. Most of the news content are also summarised by the headlines. \n",
    "\n",
    "In this notebook, we illustrate some examples of text data collection:\n",
    "- Rss feeds\n",
    "- Yelp (popular website by web scrapping)\n",
    "- Google search pages\n",
    "- Twitter (as usual!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rss Feeds\n",
    "We first illustrate with RSS Feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liming/projects/sentiment/Day1\n"
     ]
    }
   ],
   "source": [
    "# Importing packges\n",
    "# Run this first before all code\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import time\n",
    "fpath = os.getcwd()\n",
    "print (fpath)\n",
    "\n",
    "import json\n",
    "from feedparser import parse\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "TIMEOUT = 30\n",
    "jsonlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "\n",
    "def removeIndent(phrase):\n",
    "    phrase=re.sub(\"\\n\",' ',phrase)\n",
    "    phrase=re.sub(\"\\r\",' ',phrase)\n",
    "    phrase=re.sub(\"\\t\",' ',phrase)\n",
    "    return phrase\n",
    "\n",
    "def removeWS(phrase):\n",
    "    phrase=re.sub(' ','',phrase)\n",
    "    return phrase\n",
    "\n",
    "def removePunc(phrase):\n",
    "    phrase=re.sub('&',' and ',phrase)\n",
    "    phrase=re.sub(u\"\\\"\",\"\\'\", phrase)\n",
    "    phrase=re.sub(\"\\%\",\"percent\",phrase)\n",
    "  #  phrase=re.sub(',','\\,',phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of rss sites are listed below. \n",
    "- http://www.channelnewsasia.com/rss/latest_cna_biz_rss.xml # business\n",
    "- http://www.channelnewsasia.com/rss/latest_cna_sgbiz_rss.xml # sg biz\n",
    "- http://www.channelnewsasia.com/rss/latest_cna_world_rss.xml # world\n",
    "- http://www.channelnewsasia.com/rss/latest_cna_asiapac_rss.xml # asia pac\n",
    "<br>\n",
    "\n",
    "scmp\n",
    "- http://www.scmp.com/rss/2/feed  # HK\n",
    "- http://www.scmp.com/rss/10/feed  # business\n",
    "- http://www.scmp.com/rss/318421/feed # china feed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to retrieve RSS content is as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    newsurl = \"http://www.channelnewsasia.com/rss/latest_cna_world_rss.xml\"\n",
    "    if os.path.exists(\"data/cna.json\"): os.remove(\"data/cna.json\") \n",
    "    ffile = open(\"data/cna.json\",\"w\")\n",
    "    rss = parse(newsurl)\n",
    "    i = 1\n",
    "    # print (rss)\n",
    "    for rss_entry in rss['entries']:  # note format can change time to time\n",
    "        if i > 3 : break\n",
    "        i += 1\n",
    "            #     try:\n",
    "        url_link = rss_entry['id']\n",
    "        url_content = get(url_link, timeout=TIMEOUT)\n",
    "        if url_content.ok == True:                \n",
    "            page = url_content.content.decode('utf-8','ignore')\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            data = soup.find(\"div\", {\"class\": \"c-rte--article\"}).find_all('p')\n",
    "            content = \"\"\n",
    "            for element in data:\n",
    "                #print (element.text)\n",
    "                content += element.text.lstrip().rstrip()          \n",
    "            #print (content)\n",
    "            url_label = removePunc(rss_entry['title'])\n",
    "            url_id = rss_entry['id']\n",
    "            url_summary = rss_entry['summary']                  \n",
    "\n",
    "            jdata = {\"url_id\": url_id, \"content\": {\"url_label\": url_label,\"text\":content }}\n",
    "            jsonlist.append(jdata)            \n",
    "    #    except Exception as e:\n",
    "      #      pass\n",
    "        #    print (u\"Error site for \" + url_link)\n",
    "    jdata = json.dump(jsonlist, ffile)\n",
    "    ffile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection from the Yelp sites\n",
    "\n",
    "Another source of data is user-generated data, of which we look at Yelp - a popular website for restuarants and other services reviews. It is possible to obtain via their website through their API. However there are limitations if done in this manner. Here, we use web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_url = \"https://www.yelp.com/biz/the-sushi-bar-singapore?osq=Restaurants\"\n",
    "ffile = open(\"data/yelp_1.json\",\"w\")\n",
    "\n",
    "url_content = get(yelp_url)\n",
    "page = url_content.content.decode('utf-8','ignore')\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "data = soup.find(\"script\", type=\"application/ld+json\").text.lstrip().rstrip()\n",
    "data = removeIndent(data)\n",
    "\n",
    "jsondata = json.loads(data)\n",
    "json.dump(jsondata, ffile)\n",
    "\n",
    "ffile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation of web download\n",
    "Automating download of information from websites using Selenium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first clicked :0 downloaded\n",
      "Total no of reviews: 41\n",
      "no of reviews :20 downloaded\n",
      "no of reviews :40 downloaded\n"
     ]
    }
   ],
   "source": [
    "ffile = open(\"data/yelp_2.json\",\"w\")\n",
    "def getBS(data):\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    data = soup.find(\"script\", type=\"application/ld+json\").text.lstrip().rstrip()\n",
    "    data = removeIndent(data)\n",
    "    jsondata = json.loads(data)\n",
    "    return jsondata\n",
    "\n",
    "drive=webdriver.Chrome(fpath + \"/jar/chromedriver\")\n",
    "#drive=webdriver.Chrome(fpath + \"/jar/chromedriver.exe\")\n",
    "drive.set_page_load_timeout(10)\n",
    "yelp_url = \"https://www.yelp.com/biz/the-sushi-bar-singapore?start=\"\n",
    "i=0\n",
    "drive.get(yelp_url+str(i))\n",
    "time.sleep(10)\n",
    "data = drive.page_source\n",
    "data0 = getBS(data)  # in dict format\n",
    "print (\"first clicked :\" + str(i) + \" downloaded\")\n",
    "reviews = {i : data0}\n",
    "\n",
    "NbReviews = data0['aggregateRating']['reviewCount']\n",
    "print (\"Total no of reviews: \" +str(NbReviews))\n",
    "\n",
    "while i< NbReviews-20:  # code can be improved to look for next button in Selenium\n",
    "    i=i+20\n",
    "    drive.get(yelp_url+str(i))\n",
    "    print (\"no of reviews :\" + str(i) + \" downloaded\")\n",
    "    time.sleep(10)\n",
    "    data = drive.page_source\n",
    "    data = getBS(data) \n",
    "    #data = pd.DataFrame.to_json(getBS(data))  # in json format\n",
    "    reviews[i]= data \n",
    "\n",
    "#jsondata = json.loads(data0)\n",
    "json.dump(reviews, ffile)\n",
    "ffile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection from Google Search\n",
    "It is also possible to extract search snippets from google search. From then, it is a simple task to use Selenium above to extract the contents returned from the search. An example below is done for search term 'Coffee'. \n",
    "\n",
    "To do run the code below, you need to obtain an API key and also create a custom search ID from the site\n",
    "https://developers.google.com/custom-search/v1/overview?csw=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIKEY = 'AIzaSyChHbxfZWIGzhk3ogu78S0R900ZVxrNDr8'\n",
    "\n",
    "# https://developers.google.com/custom-search/v1/overview?csw=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSE_ID = 'mingsqtt'\n",
    "\n",
    "# https://developers.google.com/custom-search/v1/overview?csw=1\n",
    "# Also enable the \"Search the entire web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks something like this........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "my_api_key = APIKEY\n",
    "my_cse_id = CSE_ID\n",
    "\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://www.googleapis.com/customsearch/v1?q=macbook&cx=mingsqtt%40gmail.com&key=AIzaSyChHbxfZWIGzhk3ogu78S0R900ZVxrNDr8&alt=json returned \"Forbidden\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3578621ab632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"macbook\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cse_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f6cb646622e6>\u001b[0m in \u001b[0;36mgoogle_search\u001b[0;34m(search_term, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customsearch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeveloperKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/srsdv/lib/python3.6/site-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/srsdv/lib/python3.6/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://www.googleapis.com/customsearch/v1?q=macbook&cx=mingsqtt%40gmail.com&key=AIzaSyChHbxfZWIGzhk3ogu78S0R900ZVxrNDr8&alt=json returned \"Forbidden\">"
     ]
    }
   ],
   "source": [
    "result = google_search(\"macbook\", my_api_key, my_cse_id)\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For download of twitter feeds using Python, consider using the library tweepy. https://tweepy.readthedocs.io/en/latest/getting_started.html\n",
    "\n",
    "First create an application on Twitter. Follow the steps in https://developer.twitter.com/en/apps/ to obtain the keys belowmentioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"Use your own key etc\" \n",
    "consumer_secret = \"consumer secret\"\n",
    "access_token = \"access token\"\n",
    "access_token_secret = \"access token secret\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najib's lawyer said the court should not accept evidence given by prosecution witnesses of what Jho Low had alleged… https://t.co/gHC5Mn5cwi\n",
      "Heard on the Street: The demise of 178-year old British tour operator Thomas Cook is a testament to the profound ch… https://t.co/MU1LXhfmWN\n",
      "South Korea has culled around 15,000 pigs since the first case was reported on 17 Sept.\n",
      "\n",
      "https://t.co/yLdSoL929g\n",
      "“I’ve also got enough blood pressure medication to last me over two weeks.” A Scotsman trapped in Florida with his… https://t.co/SWIAwX6yWF\n",
      "RT @PlattsOil: Refinery Margin Tracker: Asian refining margins for US crude higher on Saudi supply hitch | #crudeoil #OOTT #refiners | http…\n",
      "RT @HumanProgress: On average commodities become 3.4% more affordable each year. That means that the time price of commodities halves every…\n",
      "RT @V_of_Europe: Sweden: Racist migrant gang films ruthless beating of young Swedish schoolboy - Voice https://t.co/hsA7lRWpMd\n",
      "Nuclear energy too slow, too expensive to save climate - report.\n",
      "#YahooFinance\n",
      "https://t.co/dY8fdCxxWC\n",
      "RT @feeonline: Such a law is merely the logical conclusion of your trade policy. https://t.co/4J0B48fv7O\n",
      "RT @RubinReport: And the winner of Twitter for today, September 23, 2019, is Dr. Gad Saad... 👏👏👏 https://t.co/GHLS1rgp7Z\n",
      "RT @JanneRiitakorpi: The same court that will decide the fate of jailed Catalan independence leaders just sided with the family of Francisc…\n",
      "RT @HumanProgress: Historically, travel was reserved to rich gentlemen of pleasure. Today, global increase of disposable incomes &amp; the decl…\n",
      "RT @Comte_dUrgell: A fairly balanced article. Gives brief, appropriate context, without assessing likelihood of Spanish state’s narrative (…\n",
      "How to make the most of your Medisave account.\n",
      "#YahooFinance\n",
      "\n",
      "https://t.co/d7hSrM4aTg https://t.co/onfDKTdnTx\n",
      "A Minnesota auto parts seller has “put everything else on the back burner” to seek relief from import duties whose… https://t.co/pvSxkqqdHE\n",
      "American shoppers so far haven’t felt much price impact from China’s soaring meat purchases, but that could soon ch… https://t.co/OsUCfVjqln\n",
      "More House Democrats called for an inquiry into reports President Trump repeatedly pressed Ukraine to probe former… https://t.co/GlQcKaY17F\n",
      "Facebook to buy brain science start-up CTRL-labs.\n",
      "#YahooFinance\n",
      "https://t.co/Vz19wpEP2I\n",
      "The Chinese technology hub of Hangzhou will assign government officials to work within 100 local companies in a new… https://t.co/d1NFaj5fmA\n",
      "Oil industry refuses to back away from fossil fuels https://t.co/9PSu8YbMUD\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This obtains tweets by the hashtag, in this case 'man utd'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "manutd = tweepy.Cursor(api.search, q='man utd').items(10)\n",
    "for tweet in manutd:\n",
    "   print (tweet.created_at, tweet.text, tweet.lang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
